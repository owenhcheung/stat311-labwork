---
title: 'STAT 311 -- Lab 3'
subtitle: 'Group 17'
author: 'Owen Cheung, Steven Tran'
date: 'Wednesday, August 3, 2022 @ 11:59pm'
output:
  html_document: default
---

<!-- This lab is due on Wednesday, August 3 by midnight. No late work is accepted. -->

<!-- Please answer ALL questions (including those that do not involve R) in the locations marked in this template. Remember to periodically **knit** your document to check that the output appears as you want it to; you will be turning in your knit html document. -->

<!-- You will need to upload this document in CANVAS. Please be sure to check that your file was uploaded correctly. It does not hurt to screenshot verification of the upload as CANVAS can be glitchy on occasion. -->

<!-- Please answer the questions in the order in which they are posed. Write in complete sentences, and support your answers where asked. -->

<!-- Throughout this assignment, PLEASE USE COMMENTS TO EXPLAIN WHAT YOUR CODE IS DOING. To make a comment in a code chunk, start the line with `#`. If you do not comment your code, you are much less likely to get partial credit if your response is incorrect. -->

<!-- The code chunk below is our setup chunk. Do not make any changes to this code. -->

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
cdc <- read.table("http://www.stat.uchicago.edu/~yibi/s220/labs/data/cdc.dat", header=TRUE)
```

* * *

## Question 1: Exercise and General Health 

<!-- This lab exercise refers to a data set from CDC. The goal is to explore the relationship between the categorical variables `genhlth`, a self reported measure of general health, and `smoke100`, an indicator for whether an individual has smoked 100 times in their life. In this assignment, we will explore the relationship between `genhlth` and `exerany`, where `exerany` is an indicator for whether or not an individual has exercised in the past month.  -->

<!-- To start, you need to load our necessary packages, load the data set, and tell R to order the 5 levels of `genhlth`. This is done in the code chunk below. -->

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(openintro)
library(statsr)
cdc <- read.table("http://www.stat.uchicago.edu/~yibi/s220/labs/data/cdc.dat", header=TRUE)
cdc <- cdc %>% mutate(genhlth = ordered(genhlth, 
            levels=c("poor", "fair", "good", "very good", "excellent"))) 
```


### Part a) 

<!-- Use `mosaic::tally()` to create a two-way contingency table between `exerany` and `genhlth`. You may add margins if you like.  -->

```{r, include=TRUE, message=FALSE}

contingency <- table(cdc$exerany, cdc$genhlth)
print.table(contingency)

contingency_margins <- addmargins(contingency)
print.table(contingency_margins)

```

### Part b) 

<!-- What proportion of the sample has exercised in the past month? What proportion of the sample reports being in excellent health? *You may use a specific function from lab to compute these proportions, or you may extract the appropriate numbers from your table in part (a) and use R as a calculator. In either case, include your code and explain how you arrived at your answer.* -->

```{r, include=TRUE, message=FALSE}

contingency <- table(cdc$exerany, cdc$genhlth)
print.table(contingency)

contingency_margins <- addmargins(contingency)
print.table(contingency_margins)

contingency_prop <- prop.table(contingency)
print(contingency_prop)


```

<!-- Use the space below to explain how you arrived at the above answer. -->

The proportion of those who have exercised in the past month which is the yes to the variable exerany is 14914/20000, which is 0.7457. The proportion of the sample reporting excellent health is 4657/20000 or 0.23285. These numbers are supported by both `contingency_prop` and `contingency_margins`. 


### Part c) 

<!-- Among those who exercised in the past month, what proportion of them report being in excellent health? What about among those who did not exercise in the past month? *There are a number of ways to arrive at these answers in R. The most direct way is to use prop.table(1) to get row percentages, but you may also use data subsetting, or extract the appropriate numbers from the contingency table and use R as a calculator. Any method is fine, but please show your work. * -->

```{r, include=TRUE}

contingency_prop <- prop.table(contingency)
print(contingency_prop)


```

<!-- Please use the space below to state your result (in a complete sentence). -->

Among the people who exersised in the past month, a proportion of about 0.19475 of them reported excellent health. Among the people who didn't exersize in the past month, only a proportion of 0.03810 of them reported excellent health. 


### Part d) 

<!-- Make a stacked bar chart that represents the two-way table in part (a). Put the ``genhlth`` variable on the x-axis and color the bar stacks using ``exerany``.  -->

```{r, include=TRUE}

bar_chart <- barplot(contingency_prop, 
                     main = "Stacked Bar chart of General Health Vs. Exercize in Last Month", 
                     sub = "Recorded by the Behavioral Risk Factor Surveillance System (BRFSS)", 
                     xlab = "genhlth", 
                     ylab = "exerany", 
                     axes = TRUE)
```

It seems as though most people would rate their health as very good, though those who exercize more make up more of the excellent, very good, good, and fair categories. The lighter grey represents those who have exercized within the past month and the darker grey represents those who didn't. I also see some response bias here where those who responded tended to have better health, which may perhaps mean that they had strong feelings for this topic. There doesn't seem to be a large 'no exercize group'. 



### Part e) 

<!-- Make a mosaic plot that compares the self-rated general health between those who had exercised in the past month and those who hadn’t. Based on the plots, which group had better self-rated general health? -->

```{r, include=TRUE}

mosaic_plot <- mosaicplot(contingency_prop, 
                          main = "Mosaic Plot of General Health Vs. Exercize in Last Month", 
                     sub = "Recorded by the Behavioral Risk Factor Surveillance System (BRFSS)")


```
Based on the plots, both the stacked bar chart and the mosaic plot, it seems as though those who exercized in the past month rated their general health higher than the population that didn't. It also seems like the people who didn't exercize may have been subjected to wording bias, maybe they felt bad for having not exercized. There is also response and non-response bias where there is a lot more responses from the people who exercized, and those who didn't exercize perhaps wouldn't have worse health than shown here on the graph. 



### Part f)

<!-- Are the two variables exerany and genhlth independent? Justify your answer using the space below. -->

No, it doesn't seem like the two variables exerany and genhealth are independent. The people who said they exercized in the past month tend to indicate a higher level of general health. However, all we can say is that there is a correlation, not a causation, because there many be many other confounding variables that may impact a person's general health. 



## Question 2: Linear Regression 

<!-- In this question, you will be repeating *all* the exercises in the Introduction to Linear Regression lab that was covered in the quiz session by the TAs. -->

<!-- To start, we will load the data for the 2011 MLB season. -->

```{r, include=TRUE}
# reading in (loading) the data set
mlb <- read.csv("https://anna-neufeld.github.io/Stat311/oiLabs/Week10/mlb11.csv")
```


### Exercise 1: 

<!-- Use `geom_point()` to make a plot that explores the relationship between runs and at_bats. -->

```{r, include=TRUE}
plot1 <- ggplot(mlb, aes(x=at_bats, y=runs)) + 
  geom_point()
print(plot1)
```

<!-- Does the relationship look linear? Base your answer on the above plot. -->
Yes this relationship looks somewhat linear because in general as x increases, y increases as well. 



### Exercise 2:

<!-- Describe the relationship between the two variables plotted above. Mention the form, direction, and strength of the relationship, as well as any unusual observations. -->

<!-- Please use the space below to state your answer. -->

I think that there seems to be a linear positive relationship between "runs" and "at bats" that is moderately strong. There may be outliers such as some of the points past 800 runs.


### Exercise 3: 

**Recall:** The correlation coefficient $r$, is calculated using the below formula.

$$r = \dfrac{1}{n} \sum_{i=1}^{n} z_x z_y = \dfrac{1}{n} \sum_{i=1}^{n} \left( \dfrac{x_i - \bar{x}}{s_x} \right) \left( \dfrac{y_i - \bar{y}}{s_y} \right)$$
This value takes a very long time to compute by hand, especially when we have a lot of observations (i.e., a very large value for $n$. It is more efficient to compute this value using a programming language like R.

<!-- Use the below code chunk to find the value of the correlation coefficient r. -->

```{r, include=TRUE}
test <- cor(mlb$runs, mlb$at_bats)
print(test)
```

<!-- Restate the value of the correlation coefficient that you found above. -->
r = 0.610627



### Exercise 4:

<!-- Run `plot_ss` several times. Choose the line that does the best job of minimizing the sum of squares. -->

<!-- Recall: the output from the plot_ss function provides you with the slope and intercept of the line as well as the sum of squares. -->

<!-- You will need to run this code in the console because the plot_ss function is interactive. -->

<!-- What slope and intercept corresponded to this "best" line? Write the equation in R using the same formatting as in exercise 3. -->
coefficients: -2370.547, 0.556
sum of squares: 126186.4

$$y = 0.556x - 2370.547$$


### Exercise 5:

<!-- Quiz Question: The Seattle Mariners had 5421 at bats in 2011. Suppose that you had access to the regression line but not the actual data. How many runs would you predict that the Mariners would score in 2011? -->

<!-- You may use R as a calculator when answering this question, but it is not required. -->

```{r, echo=FALSE, results="hide"}
ex5 <- 0.556*5421 - 2370.547
print(ex5)
```

<!-- Please state your answer using the space below. -->
644 runs



### Exercise 6:

<!-- Was you answer to Exercise 5 an underestimate or an overestimate? In order to explain your answer, use the regression line to calculate the residual. You must show your work, meaning your code should appear in your knitted document. -->

```{r, include=TRUE}
residual <- 556 - 643.529
print(residual)
```

<!-- Please state your answer using the space below. -->
Since the residual was negative the answer to exercise 5 was an overestimate.



### Exercise 7:

<!-- Using `lm`, fit a new model that uses `homeruns` to predict `runs`. Using the estimates from the R output, write the equation of the regression line.  -->

```{r, include=TRUE}
ex7 <- lm(runs ~ homeruns, data = mlb)
summary(ex7)

```

<!-- Write out the equation for the regression line using the same formatting as in Exercise 3. -->
$$y = 1.8345x + 415.2389$$



### Exercise 8:

<!-- Quiz Question: What proportion of variability in `runs` is explained by variation in `homeruns`? -->

```{r, echo=FALSE, results="hide"}
```

<!-- Please state your answer using the space below. -->
From adjusted r-squared we can say that 61.32% of variability in runs was able to be explained.



### Exercise 9:

<!-- Quiz Question: Which of the following variables seems best to predict `runs`?
<!-- (1) `stolen_bases` -->
<!-- (2) `new_obs` -->
<!-- (3) `new_slug` -->
<!-- (4) `homeruns` -->

<!-- You may use R-squared as your criteria. You should use code to support your answer. -->

```{r, include=TRUE}
ex8_1 <- lm(runs ~ stolen_bases, data = mlb)
summary(ex8_1)


ex8_2 <- lm(runs ~ new_obs, data = mlb)
summary(ex8_2)
ex8_3 <- lm(runs ~ new_slug, data = mlb)
summary(ex8_3)


ex8_2 <- lm(runs ~ new_obs, data = mlb)
summary(ex8_2)

ex8_3 <- lm(runs ~ new_slug, data = mlb)
summary(ex8_3)


ex8_4 <- lm(runs ~ homeruns, data = mlb)
summary(ex8_4)
```

<!-- Please state your answer using the space below. -->
new_obs seems best to predict runs as the adjusted r-squared value of 93.26% is the highest out of all four cases which means more of the variability was able to be explained.


### Exercise 10

<!-- Show appropriate visual and numerical summaries of the model that you chose in the previous exercise. -->

```{r, include=TRUE}
ex10 <- lm(runs ~ new_obs, data = mlb)
summary(ex10)

```

```{r, include=TRUE}
# visual summaries
ex10_graph <- ggplot(mlb, aes(x=new_obs, y=runs)) + geom_point() + stat_smooth(method="lm", se=FALSE)
print(ex10_graph)

```

<!-- Describe your model based on the above information. -->
This model seems to show a positive linear relationship between new_obs and runs that is fairly strong. There do not seem to be any outliers.



### Exercise 11

<!-- Quiz Question: Which of the following variables had a negative relationship with `runs`? Do you think this answer makes sense? -->


<!-- (1) wins -->
<!-- (2) strikeouts -->
<!-- (3) bat_avg -->
<!-- (4) hits -->

```{r, include=TRUE}
ex11_1 <- lm(runs ~ wins, data = mlb)
summary(ex11_1)


ex11_2 <- lm(runs ~ strikeouts, data = mlb)
summary(ex11_2)
ex11_3 <- lm(runs ~ bat_avg, data = mlb)
summary(ex11_3)


ex11_2 <- lm(runs ~ strikeouts, data = mlb)
summary(ex11_2)

ex11_3 <- lm(runs ~ bat_avg, data = mlb)
summary(ex11_3)


ex11_4 <- lm(runs ~ hits, data = mlb)
summary(ex11_4)
```

<!-- Please state your answer in the space below. -->
Strikeouts had a negative relationship with runs. This makes sense because if the player strikes out they wouldn't run.



### Exercise 12

<!-- Show appropriate visual and numerical summaries of the model that you chosen in the previous exercise. -->

```{r, include=TRUE}
# numerical summaries
ex12 <- lm(runs ~ strikeouts, data = mlb)
summary(ex12)
```

```{r, include=TRUE}
# visual summaries
ex12_graph <- ggplot(mlb, aes(x=strikeouts, y=runs)) + geom_point() + stat_smooth(method="lm", se=FALSE)
print(ex12_graph)
```

<!-- Describe your model based on the above information. -->

This model seems to show a negative linear relationship between strikeouts and runs that is somewhat weak. There may be outliers under 600 runs.



## Question 3: Type 1 and Type 2 errors

<!-- J & T each want to test if the proportion of adults in their neighborhood who have graduated from high school is 0.94, as claimed in the newspaper. -->

<!-- J takes a random sample of 200 adults and uses $alpha=0.05$. T takes a random sample of 500 adults and uses $alpha=0.05$. Suppose the newspaper's percentage is actually right. -->


Null hypothesis: newspaper percentage is right
Alternate hypothesis: newspaper percentage is false

Type 1 error: reject null hypothesis when null is true
Type 1 Error: Called alpha Only depends on the value of alpha, and sample size doesn’t matter


Type 2 error: reject alternate hypothesis when alternate is true
Type 2 error: This is called beta, this error rate decreases as your sample size increases


### Part a(i)
<!-- Is it possible for either J or T to make a type 1 error? If yes, who is more likely to do so? If no, why not? -->

Yes, it is possible for either J or T to make a type 1 error. For J and T, this would mean rejecting the null hypothesis that the proportion of adults in their neighborhood who have graduated from high school is equal to 0.94 when that proportion is indeed true. 

Both are equally likely to make a type 1 error since this error only depends on the value of $alpha and the sample size doesn't really matter in this case. 


### Part a(ii)

<!-- Is it possible for either J or T to make a type 2 error? If yes, who is more likely to do so? If no, why not? Use the space below to give your response. -->

Yes, it is possible for either J or T to make a type 2 error. This means that J and T reject the alternative hypothesis that the proportion of adults in their neighborhood who've graduated from high school does not equal to 0.94, when the proportion truly isn't equal to 0.94. 

J is more likely to make a type 2 error since his sample size of 200 is a lot smaller than T's sample size of 500. This is because the error rate decreases as the sample size increases. 


<!-- Now consider: -->

<!-- J & T each want to test if the proportion of adults in their neighborhood who took chemistry in high school is 0.25, as claimed in the newspaper.  -->

<!-- J takes a random sample of 200 adults and uses $alpha=0.05$. T takes a random sample of 500 adults and uses $alpha=0.05$. Suppose the newspaper's percentae is actually wrong. -->


# Null hypothesis: newspaper percentage is right
# Alternate hypothesis: newspaper percentage is false

# Type 1 error: reject null hypothesis when null is true
# Type 1 Error: Called alpha Only depends on the value of alpha, and sample size doesn’t matter


# Type 2 error: reject alternate hypothesis when alternate is true
# Type 2 error: This is called beta, this error rate decreases as your sample size increases



### Part b(i)

<!-- Is it possible for either J or T to make a type 1 error? If yes, who is more likely to do so? If no, why not? -->

It is not possible for either J or T to make a Type 1 error since we know that the null hypothesis that the newspaper's proportion of adults in their neighborhood who took chemistry of 0.25 is incorrect in the first place so even if we reject the null hypothesis, it isn't true to begin with to make that error. 

Type 1 error is also dependent in $alpha and not sample size so since they both have the same alpha value, they have equal likelihood of commiting an error if it was possible. It isn't in this case. 


### Part b(ii) 

<!-- Is it possible for either J or T to make a type 2 error? If yes, who is more likely to do so? If no, why not? -->


Yes, it is possible for either J or T to make a type 2 error. This means that J and T reject the alternative hypothesis that the proportion of adults in their neighborhood who've taken high school chemostry from high school does not equal to 0.25, when the proportion truly isn't equal to 0.94. 

J is more likely to make a type 2 error since his sample size of 200 is a lot smaller than T's sample size of 500. This is because the error rate decreases as the sample size increases. 


